\subsection{\Model}
\label{subsec:afm-guidance}

Compared with previous simulation-based approaches, \Model offers a significant advantage of computational cost. 
Specifically, by relying on the diffusion process of \AFiii, which exploits co-evolutionary information, it is possible to generate conformations consistent with AFM images at substantially reduced computational cost. 

On the other hand, this approach transforms AFM images into inter-domain distance via the CNN, that are easy to compute even for the structure in the generation process of \AFiii, rather than uses the raw images directly. 
During this transformation, the pixel-level height information is compressed into low-dimensional inter-domain distances. 
As a result, the rich structural information inherently present in AFM images cannot be fully utilized, which imposes limitations on prediction accuracy. 
Therefore, improving the accuracy of \Model requires improving the prediction accuracy of the CNN itself, to preserve as much structural information as possible during this compression.

In this section, we introduce the concrete procedure of \Model, with a particular focus on the CNN training strategy, which plays a critical role in determining the overall accuracy of the method.

\subsubsection{Preparation}
\label{subsubsec:afm-prep}

To train the CNN to learn the relationship between AFM images and underlying 3D structures, we construct training data by generating pseudo-AFM images from 3D conformations and pairing them with ground-truth inter-domain distance labels. 
In \Model, these conformations are obtained from a candidate set generated by applying diverse restraints to \AFiii, not from simulation trajectories.
This strategy offers two key advantages. 

First, the computational resources and expertise required are substantially reduced. 
Sampling diverse domain motions through molecular simulations demands considerable computational resources and extensive time, 
and also, it typically requires expert knowledge to conduct properly. 
In contrast, leveraging \AFiii enables many users to generate candidate structures with only limited computational resources. 

Second, this approach allows for sufficient sampling of intermediate conformations so that the CNN can interpolate structural hypotheses. 
In order to detect smooth conformational transitions, the CNN requires training data about not only two boundary states (e.g., start and end conformations) but also representative intermediate conformations linking them. 
In the absence of such intermediates, the CNN is unable to accurately model continuous structural variability.
By leveraging machine learning–based approaches for molecular conformation generation, it becomes possible to deliberately sample such intermediate conformations, which would be difficult to obtain through conventional simulations.

\paragraph{Generating candidate conformations to broadly cover $\phi$-space}
Here, we explain how we sample diverse conformations by \AFiii, broadly covering large-scale domain-level rearrangements underlying the AFM observations. 
%If the coverage is too narrow, the CNN may fail to learn a stable AFM$\rightarrow \phi$ mapping for unseen configurations. 
%In this sense, this exploration corresponds to formulating hypotheses for predicting inter-domain distances from AFM images.
The target inter-domain distance restraints for \AFiii are composed as below.

First, a reference structure $X_{\text{ref}}$ is obtained with \AFiii (with no restraints).
Then, target vectors $\{\phi_{\text{perturb}}^{\,i}\}$ are placed on a grid around $\phi(X_{\text{ref}})$ such that, for each axis $d$,

\begin{equation}
    \phi_{\text{perturb},d}^i \in \bigl[ 0, 4.0 \times \phi(X_{\text{ref}})_d \bigr], \qquad \forall i.
    \label{eq:target_domain_distance}
\end{equation}

The grid uses a step of $0.6~\mathrm{nm}$ for each axis. 
Then, the generation with \AFiii is conducted using each $\{\phi_{\text{perturb}}^{\,i}\}$ as the restraints.

%\paragraph{Early rejection criteria and stopping strategy}
%In the generation step just before, although candidate conformations can span a wide range of $\phi$-space, the number of grid points grows rapidly with the dimensionality (i.e., the number of chosen domain pairs). 
%While diversity is important, many unphysical conformations will be filtered out later during sanitization, so it is preferable to avoid generating such conformations in the first place. 

%We therefore defined a conformation as \emph{invalid} if fewer than $92\%$ of its C$\alpha$–C$\alpha$ bond lengths fall within $[0.37,0.39]~\mathrm{nm}$. 
%Conformations are generated with restraint targets sorted by increasing distance from $\phi(X_{\text{ref}})$, starting from $\phi_{\text{perturb}}=\phi(X_{\text{ref}})$. 
%The exploration is terminated once all previously generated candidates within a three-step neighborhood on the grid (i.e., nodes reachable within at most three edges) are invalid.

\paragraph{Geometric sanitization.}
It should be noted that the candidate conformation set generated in the previous step may contain geometric violations (e.g., steric clashes, backbone/side-chain outliers). 
Such conformations could potentially bias the CNN by introducing unphysical patterns without experimental counterparts. 
Considering that it is still important to maintain the diversity in the training set, 
we applied permissive thresholds (\cref{tab:score_thresholds}) to perform geometric sanitization, excluding only clear violations. 
This procedure constrains the CNN to perform inference within a conformational space devoid of geometric violations.
The resulting set that passes this gate serves as a reliable population for synthesizing pseudo-AFM images—the inputs used in training.

\begin{table}[H]
\centering
\caption{MolProbity thresholds for candidate conformations.}
\label{tab:score_thresholds}
\begin{tabular}{|l|c|}
\hline
Criterion & Threshold \\
\hline
MolProbity Score & $\leq 10.0$ \\
Clash Score & $\leq 13.0$ \\
Ramachandran favored (\%) & $\geq 90.0$ \\
Rotamer Outlier (\%) & $\leq 50.0$ \\
\hline
\end{tabular}
\end{table}

\paragraph{Pseudo-AFM image rendering.}
In rendering the pseudo-AFM, we used the function implemented in \cite{matsunaga2023endtoend}. 
%This function was chosen owing to its torch-based implementation, which facilitates parallelization and enables efficient computation.

%Each sanitized conformation is subsequently rendered into a pseudo-AFM image following the procedure described in \cite{matsunaga2023endtoend}. 
The rendering is performed as follows. 
First, each conformation is randomly rotated and translated such that its minimum $z$-coordinate is set to $0$, as the molecule is placed on the stage. 
%Next, the conformation is uniformly randomized in the $xy$-plane within the image boundaries while ensuring that the entire molecule remains within the frame. 
A virtual AFM probe then scans the structure, and the probe’s vertical displacement is recorded, corresponding to a morphological dilation. 
The resulting displacement map directly constitutes the pseudo-AFM image.
The specific parameter settings are summarized in \cref{tab:afm_settings}. 
In particular, since the precise tip geometry is unknown in experiments, the probe radius $r$ and taper angle $a$ are sampled uniformly within predefined ranges to emulate realistic variability. 
Finally, for \flhac, the function \verb|skimage.exposure.match_histograms| is applied to match each generated image histogram to that of the experimental frames.

\begin{table}[htbp]
\centering
\caption{Settings for training AFM images.}
\label{tab:afm_settings}
\begin{tabular}{|l|c|c|}
\hline
 & \AK & \flhac \\
\hline
Resolution [nm/pixel] & 0.3 & 0.98 \\
Image Size [pixel $\times$ pixel] & $35 \times 35$ & $35 \times 35$ \\
Tip Radius [nm] & $r \sim \mathrm{Uniform}(1,2)$ & $r \sim \mathrm{Uniform}(2,6)$ \\
Tip Angle [degree] & $a \sim \mathrm{Uniform}(10,30)$ & $a \sim \mathrm{Uniform}(10,30)$ \\
Noise Std. Dev. [nm] & 0.0 & 0.5 \\
Histogram Matching & --- & \checkmark \\
Dataset Size [frames] & 5M & 5M \\
\hline
\end{tabular}
\end{table}

\subsubsection{Training}
\label{subsubsec:afm-training}

The training of the CNN in \Model can be regarded as supervised regression, where the task is to predict the continuous domain distances. 
As the most straightforward loss function for supervised regression, we adopted the mean squared error (MSE) loss in the domain-distance space:

\begin{equation}
    \mathcal{L}_{\text{CNN}} = \sum_{i} \| \phi_{\text{pred}}(I^i) - \phi_{\text{true}}^i \|,
    \label{eq:cnn-mse-loss}
\end{equation}

where $I^i$ denotes the $i$-th input image, $\phi_{\text{pred}}(I^i)$ is the domain distance predicted by the CNN, and $\phi_{\text{true}}^i$ is the corresponding ground-truth domain distance.
As the CNN architecture, we implemented a $(\mathbb{R}^2, +) \rtimes C_8$-invariant CNN (see \cref{alg:cnn} for the detailed implementation).

All models in this paper were trained on a single node equipped with a NVIDIA RTX A6000 GPU (48~GB memory). 
The training took approximately 5 hours for the \AK dataset, whereas it required about 64 hours for \flhac. 
The loss convergence for \flhac took significantly longer, but our empirical observation suggests that the training time tends to depend on the range of the tip radius and the noise level.

\subsubsection{Inference}
\label{subsubsec:afm-inference}

An important aspect in reconstructing conformations from AFM images is that the data are inherently time series. 
By analyzing AFM images as a temporal sequence, one can visualize domain fluctuations and morphing pathways of proteins. 
This perspective allows them to be interpreted as parts of continuous transitions, 
and provides the opportunity to establish connections with theoretical frameworks such as molecular simulations or Markov state models. 
Accordingly, computational efficiency that enables the rapid processing of multiple frames is essential.

During inference in \Model, the inter-domain distances predicted by the CNN from AFM images are employed as restraints to guide structure generation with \AFiii. 
In this study, we required the generated structures to achieve a squared error between the CNN's prediction in inter-domain distances below 3~nm. 
Notably, for both \AK and \flhac, the inference was completed in less than one minute.

%In this study, because the guidance scheduling $\eta_t$ was adjusted, the inter-domain distances of generated 
%conformations often deviated from the restraints. To address this issue, instead of directly adopting the CNN's output as $\phi_{\text{target}}$, we set restrictions based on past predictions, typically the conformation pool created during training data generation. 

\subsubsection{Evaluation}
\label{subsubsec:afm-evaluation}

To evaluate how well the predicted conformation reproduces the reference image, we performed a rigid-body fitting, comprising 60{,}000 random rotations uniformly sampled from $\mathrm{SO}(3)$.
For each rotation $R \in \mathrm{SO}(3)$, we (i) rotated the structure by $R$, (ii) aligned its $xy$ coordinates to the image centroid, and (iii) translated it in both $x$ and $y$ over $[-5,5]$ nm with 0.5 nm increments.

At each translated pose, we generated a pseudo-AFM image and computed its the correlation coefficient (c.c.) with the reference image, using \cref{eq:cc}.
For each rotation, we recorded the pose with the maximum c.c. over translations.
Finally, the pose achieving the overall maximum c.c. across all rotations and translations was taken as the optimal pose.

\begin{equation}
    \mathrm{c.c.}(R)=
    \frac{\sum_{p \in \text{pixels}} H^{\text{(exp)}}_p H^{\text{(sim)}}_p(R)}
         {\sqrt{\sum_{p \in \text{pixels}} \left(H^{\text{(exp)}}_p\right)^2}
          \sqrt{\sum_{p \in \text{pixels}} \left(H^{\text{(sim)}}_p(R)\right)^2}}
    \label{eq:cc}
\end{equation}

where $H^{\text{(exp)}}_p$ and $H^{\text{(sim)}}_p(R)$ denote the pixel intensities of the experimental and generated images (transformed by $R$), respectively, and the summation runs over all pixels $p$.
